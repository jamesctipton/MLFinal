{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8945012",
   "metadata": {},
   "source": [
    "# Analyzing Political Tweets on a Depression Prediction ML Model\n",
    "### Sam Spell, James Tipton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f656d",
   "metadata": {},
   "source": [
    "Political rhetoric and discussions have seemingly become more polarized recently. In history and while reaching adulthood, being able to vote and be a part of politics is a very important role in a stable and healthy society. This project aims to use machine learning to develop a model to predict depression based on a string of text from twitter. Once this model is developed, it can be used to conduct an analysis on political messages sent online. We will be able to draw out patterns in twitter texts that the machine learning model classifies as showing signs of Depression. Another goal of this machine learning model is to extract patterns of text that can be connected to patterns of political messaging if they exist, and to compare this to a temporal aspect. With the changing view on polarized politics, it will be interesting to test if there is a change in the prevalence of messages classified with “depression” throughout different political times.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5979c49",
   "metadata": {},
   "source": [
    "#### Step 1: Clean the datasets to prepare for the model\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e48d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb6ce85",
   "metadata": {},
   "source": [
    "run these downloads once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55651476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84be90ef",
   "metadata": {},
   "source": [
    "filter for stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576ac5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate text column of dataset\n",
    "d = pd.read_csv(\"depression.csv\")\n",
    "text = d[\"clean_text\"]\n",
    "\n",
    "# determine stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_text = \" \".join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "text = text.apply(remove_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32dbdf50",
   "metadata": {},
   "source": [
    "lemmatize and stem each reddit post in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672c793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "text = text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010b185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_text = \" \".join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "\n",
    "text = text.apply(stem_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4a0513",
   "metadata": {},
   "source": [
    "cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3519abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(text, d['is_depression'], test_size=0.2, random_state=42)\n",
    "\n",
    "# convert phrases into numerical vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7f1620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train SVM model\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec150b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9521654815772462\n",
      "Precision: 0.9563492063492064\n",
      "Recall: 0.9463350785340314\n",
      "F1 score: 0.9513157894736843\n"
     ]
    }
   ],
   "source": [
    "# evaluate SVM model\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1 score:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0ddde02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2423    0\n",
      "2424    0\n",
      "2425    0\n",
      "2426    0\n",
      "2427    0\n",
      "Length: 2428, dtype: int64\n",
      "2        Who watched the state of the union last night ...\n",
      "11       I have fallen for this trap several times and ...\n",
      "20       One of the things I have noticed in todays wor...\n",
      "42       [https://kites-journal.org/2022/03/01/between-...\n",
      "54       ***\"Axe tax\"*** aka ***\"Hammer tax\"*** aka ***...\n",
      "                               ...                        \n",
      "12829    \"Well now, which is the longest river in Afric...\n",
      "12837    Let's say there are private courts/resolution ...\n",
      "12838    Basically, the above. It can be a very effecti...\n",
      "12842    Last week I would've considered myself a liber...\n",
      "12853    I go to the mises.org and listen to the writin...\n",
      "Name: Text, Length: 2428, dtype: object\n"
     ]
    }
   ],
   "source": [
    "p = pd.read_csv(\"political.csv\")\n",
    "new_text = pd.Series.dropna(p[\"Text\"])\n",
    "\n",
    "x_new = vectorizer.transform(new_text)\n",
    "\n",
    "y_new_pred = clf.predict(x_new)\n",
    "\n",
    "print(y_new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9a7ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14168039538715 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"{(sum(y_new_pred) / 2428) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "84d98050c291fbabc1a867c10b51e932498cf5095538e7bdaecff4b39fb1caa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
